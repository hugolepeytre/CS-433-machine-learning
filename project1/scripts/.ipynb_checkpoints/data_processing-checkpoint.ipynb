{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:55:45.436988Z",
     "start_time": "2020-10-19T15:55:43.734351Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:55:53.464283Z",
     "start_time": "2020-10-19T15:55:45.450231Z"
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:55:53.503819Z",
     "start_time": "2020-10-19T15:55:53.466371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,) (250000, 30) (250000,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape, tX.shape,ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:55:53.552034Z",
     "start_time": "2020-10-19T15:55:53.512282Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_nan(tX):\n",
    "    \"\"\"\n",
    "    Label the -999 values as NaN\n",
    "    \"\"\"\n",
    "    tX_copy = np.copy(tX)\n",
    "    tX_copy[tX_copy == -999] = np.nan\n",
    "    return tX_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:55:53.598662Z",
     "start_time": "2020-10-19T15:55:53.556039Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_empty_columns(tX, threshold = 0.4):\n",
    "    \"\"\"\n",
    "    Remove feature columns containing more than the specified threshold proportion of NaN\n",
    "    \"\"\"\n",
    "    tX_copy = np.copy(tX)\n",
    "    #For each column compute the ratio of nan values over the number of rows\n",
    "    prop_empty_column = (np.isnan(tX_copy)).sum(axis=0) / len(tX_copy)\n",
    "    \n",
    "    column_mask = prop_empty_column < threshold\n",
    "    return tX_copy[:, column_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:55:53.640377Z",
     "start_time": "2020-10-19T15:55:53.603422Z"
    }
   },
   "outputs": [],
   "source": [
    "def copy_data(y, tX, ids):\n",
    "    return np.copy(y), np.copy(tX) , np.copy(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:55:53.680907Z",
     "start_time": "2020-10-19T15:55:53.644002Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_nan(y, tX, ids, remove=True, replace_val=0):\n",
    "    \"\"\"\n",
    "    Filter the nan (-999) values, by either removing the rows or replacing by the specified replace_val.\n",
    "    \"\"\"   \n",
    "    y_copy, tX_copy, ids_copy = copy_data(y, tX, ids)\n",
    "    mask = np.isnan(tX_copy)# True if Nan, False otherwise\n",
    "    \n",
    "    if remove:\n",
    "        # Remove the rows containing any NaN\n",
    "        row_mask = ~mask.any(axis=1) # sets to False any rows containing NaN\n",
    "        tX_copy, y_copy, ids_copy = tX_copy[row_mask], y_copy[row_mask], ids_copy[row_mask]\n",
    "    else:\n",
    "        #Replace NaN values by replace_val\n",
    "        tX_copy[mask] = replace_val\n",
    "        \n",
    "    return y_copy, tX_copy, ids_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T16:18:16.031247Z",
     "start_time": "2020-10-19T16:18:15.979734Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_outliers(y, tX, ids):\n",
    "    \"\"\"\n",
    "    Remove outliers feature points using Interquartile range.\n",
    "    \"\"\"\n",
    "    print(\"\"\"TODO: only remove outliers when max - min > threshold like 10\n",
    "    Doubt with DER_deltar_tau_lep and PRI_jet_all_pt\"\"\")\n",
    "    y_copy, tX_copy, ids_copy = copy_data(y, tX, ids)\n",
    "    # Compute first and third quartiles and the Interquartile range\n",
    "    Q1 = np.percentile(tX_copy, 25, axis=0) \n",
    "    Q3 = np.percentile(tX_copy, 75, axis=0)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = (tX_copy >= Q1 - 1.5 * IQR) & (tX_copy <= Q3 + 1.5 * IQR) # set to True any entry outside the interquartile range\n",
    "\n",
    "    row_mask = mask.all(axis=1) #sets to False rows containing any outliers\n",
    "    return y_copy[row_mask], tX_copy[row_mask], ids_copy[row_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:59:28.972922Z",
     "start_time": "2020-10-19T15:59:28.938331Z"
    }
   },
   "outputs": [],
   "source": [
    "def noncategorical_columns(tX):\n",
    "    \"\"\"\n",
    "    Computes the columns with more that 10 unique values\n",
    "    \"\"\"\n",
    "     # count the number of unique values\n",
    "    nunique_col = (np.diff(np.sort(tX, axis=0), axis=0) != 0).sum(axis=0) + 1 \n",
    "    noncategorical_col = nunique_col > 10 #set to True columns with more than 10 unique elements\n",
    "    return noncategorical_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T15:59:29.703733Z",
     "start_time": "2020-10-19T15:59:29.661670Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale(tX, method=\"standard\"):\n",
    "    \"\"\"\n",
    "    Scale noncategorical features using the specified method. Possible methods: standard, min-max\n",
    "    \"\"\"\n",
    "    tX_copy = np.copy(tX)\n",
    "    noncategorical_col = noncategorical_columns(tX_copy)\n",
    "    tX_noncat = tX_copy[:,noncategorical_col]\n",
    "    \n",
    "    if method == \"standard\": \n",
    "        #Standardize the data\n",
    "        tX_copy[:,noncategorical_col] = (tX_noncat - tX_noncat.mean(axis=0)) / tX_noncat.std(axis=0) \n",
    "    else:\n",
    "        #Apply a min-max normalization to scale data between 0 and 1\n",
    "        col_min = tX_noncat.min(axis=0)\n",
    "        col_max = tX_noncat.max(axis=0)\n",
    "        tX_copy[:,noncategorical_col] = (tX_noncat - col_min) / (col_max - col_min)\n",
    "    return tX_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:00:41.049754Z",
     "start_time": "2020-10-19T17:00:41.006273Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_correlated_features(tX, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Compute the correlations between each feature and remove features that have a correlation greater\n",
    "    than the specified threshold\n",
    "    \"\"\"\n",
    "    tX_copy = np.copy(tX)\n",
    "    noncategorical_col = noncategorical_columns(tX_copy)\n",
    "    cat_idx = np.where(~noncategorical_col)[0] # index of non categorical features\n",
    "    tX_noncat = tX_copy[:,noncategorical_col]\n",
    "    \n",
    "    corr_matrix = np.corrcoef(tX_noncat, rowvar=False)\n",
    "    \n",
    "    #set to False highly correlated columns\n",
    "    nb_col = len(corr_matrix)\n",
    "    columns = np.full((nb_col,), True, dtype=bool)\n",
    "    for i in range(nb_col):\n",
    "        for j in range(i+1, nb_col):\n",
    "            if corr_matrix[i,j] >= threshold:\n",
    "                if columns[i]:\n",
    "                    columns[j] = False\n",
    "     \n",
    "    #remove correlated features and concat categorical features\n",
    "    return np.c_[tX_noncat[:,columns],tX_copy[:,cat_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T16:47:39.901810Z",
     "start_time": "2020-10-19T16:47:39.847202Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_pri_colums(tX):\n",
    "    pass\n",
    "\n",
    "def remove_der_columns(tX):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:00:42.030030Z",
     "start_time": "2020-10-19T17:00:41.988499Z"
    }
   },
   "outputs": [],
   "source": [
    "def pipeline(y, tX, ids):\n",
    "    tX_nan = set_nan(tX)\n",
    "    tX_columns = remove_empty_columns(tX_nan)\n",
    "    y_filtered, tX_filtered, ids_filtered = filter_nan(y, tX_columns, ids)\n",
    "    y_outliers, tX_outliers, ids_outliers = remove_outliers(y_filtered, tX_filtered, ids_filtered)\n",
    "    tX_scale = scale(tX_outliers, method=\"standard\")\n",
    "    tX_corr = remove_correlated_features(tX_scale, threshold=0.9)\n",
    "    return tX_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:00:43.734564Z",
     "start_time": "2020-10-19T17:00:42.707556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: only remove outliers when max - min > threshold like 10\n",
      "    Doubt with DER_deltar_tau_lep and PRI_jet_all_pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.00116781,  0.60294873,  1.27197329, ...,  1.16307234,\n",
       "         0.25544962,  2.        ],\n",
       "       [ 1.79100178,  1.23888656,  1.54716042, ...,  0.39352041,\n",
       "         0.64881421,  1.        ],\n",
       "       [-0.711809  , -0.81307483, -0.69616427, ..., -1.3005739 ,\n",
       "        -0.34892147,  3.        ],\n",
       "       ...,\n",
       "       [-0.72007049,  1.88954019,  0.35822785, ..., -1.53764989,\n",
       "         1.62671638,  1.        ],\n",
       "       [ 0.82493433,  1.56486369,  0.82224994, ..., -0.66441097,\n",
       "         1.39973289,  1.        ],\n",
       "       [-0.15941388,  0.93260481,  0.15310963, ...,  0.97405958,\n",
       "        -0.08061817,  1.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(y, tX, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
