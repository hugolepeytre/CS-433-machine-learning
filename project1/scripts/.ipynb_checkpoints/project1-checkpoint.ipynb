{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T13:26:46.207940Z",
     "start_time": "2020-10-19T13:26:42.514184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T13:26:46.244520Z",
     "start_time": "2020-10-19T13:26:46.211169Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../data/'\n",
    "DATA_ZIP = DATA_FOLDER + 'datasets.zip'\n",
    "\n",
    "DATA_TRAIN_PATH = DATA_FOLDER + 'train.csv'\n",
    "DATA_TRAIN_PATH_CLEAN = DATA_FOLDER + 'train_clean.csv'\n",
    "DATA_TEST_PATH = DATA_FOLDER + 'test.csv' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:08:47.760766Z",
     "start_time": "2020-10-19T18:08:38.177337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: only remove outliers when max - min > threshold like 10\n",
      "    Doubt with DER_deltar_tau_lep and PRI_jet_all_pt\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False]\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "from split_data import *\n",
    "from data_processing import *\n",
    "\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "y_clean, tX_clean, ids_clean = clean_training(y, tX, ids)\n",
    "\n",
    "\n",
    "split_ratio = 0.8\n",
    "tX_train, tX_validation, y_train, y_validation = split_data(tX, y, split_ratio)\n",
    "tX_train_clean, tX_validation_clean, y_train_clean, y_validation_clean = split_data(tX_clean, y_clean, split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:09:00.273121Z",
     "start_time": "2020-10-19T18:09:00.241655Z"
    }
   },
   "outputs": [],
   "source": [
    "# from proj1_helpers import *\n",
    "# from split_data import *\n",
    "\n",
    "# y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "# y_clean, tX_clean, ids_clean = load_csv_data(DATA_TRAIN_PATH_CLEAN)\n",
    "\n",
    "# split_ratio = 0.8\n",
    "# tX_train, tX_validation, y_train, y_validation = split_data(tX, y, split_ratio)\n",
    "# tX_train_clean, tX_validation_clean, y_train_clean, y_validation_clean = split_data(tX_clean, y_clean, split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:09:01.148997Z",
     "start_time": "2020-10-19T18:09:01.111709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw set : \n",
      "               Training       Validation     \n",
      "Features       (200000, 30)   (50000, 30)    \n",
      "Labels         (200000,)      (50000,)       \n",
      "\n",
      "Clean set : \n",
      "               Training       Validation     \n",
      "Features       (35536, 29)    (8884, 29)     \n",
      "Labels         (35536,)       (8884,)        \n"
     ]
    }
   ],
   "source": [
    "print(\"Raw set : \")\n",
    "row_format = \"{:<15}\" * 3\n",
    "print(row_format.format(\"\", \"Training\", \"Validation\"))\n",
    "print(row_format.format(\"Features\", str(tX_train.shape), str(tX_validation.shape)))\n",
    "print(row_format.format(\"Labels\", str(y_train.shape), str(y_validation.shape)))\n",
    "\n",
    "print(\"\\nClean set : \")\n",
    "row_format = \"{:<15}\" * 3\n",
    "print(row_format.format(\"\", \"Training\", \"Validation\"))\n",
    "print(row_format.format(\"Features\", str(tX_train_clean.shape), str(tX_validation_clean.shape)))\n",
    "print(row_format.format(\"Labels\", str(y_train_clean.shape), str(y_validation_clean.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:09:05.941944Z",
     "start_time": "2020-10-19T18:09:04.877138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34483741738725987\n",
      "0.34177970796002916\n"
     ]
    }
   ],
   "source": [
    "from stochastic_gradient_descent import *\n",
    "from gradient_descent import *\n",
    "from costs import *\n",
    "\n",
    "\n",
    "max_iters = 100\n",
    "gamma = 0.01\n",
    "initial_w = np.zeros(tX_train_clean.shape[1])\n",
    "\n",
    "loss, weights_gd = gradient_descent(y_train_clean, tX_train_clean, initial_w, max_iters, gamma)\n",
    "print(loss)\n",
    "loss, weights_sgd = stochastic_gradient_descent(y_train_clean, tX_train_clean, initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:09:07.454687Z",
     "start_time": "2020-10-19T18:09:07.408797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD Training loss : 0.3447192031684568\n",
      "GD Validation loss : 0.34342161403433336\n",
      "SGD Training loss : 0.345007073088848\n",
      "SGD Validation loss : 0.3434787185463747\n"
     ]
    }
   ],
   "source": [
    "# Not compared with the not cleaned set, because it needs normalizing to work\n",
    "print(\"GD Training loss : {}\".format(compute_loss(y_train_clean, tX_train_clean, weights_gd)))\n",
    "print(\"GD Validation loss : {}\".format(compute_loss(y_validation_clean, tX_validation_clean, weights_gd)))\n",
    "print(\"SGD Training loss : {}\".format(compute_loss(y_train_clean, tX_train_clean, weights_sgd)))\n",
    "print(\"SGD Validation loss : {}\".format(compute_loss(y_validation_clean, tX_validation_clean, weights_sgd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:09:09.133569Z",
     "start_time": "2020-10-19T18:09:09.080822Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from least_squares import *\n",
    "\n",
    "weights, t_loss = least_squares(y_train, tX_train)\n",
    "weights_clean, t_loss_clean = least_squares(y_train_clean, tX_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:09:09.609405Z",
     "start_time": "2020-10-19T18:09:09.569175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss : 0.3394268495585387\n",
      "Validation loss : 0.3407678460856664\n",
      "Clean training loss : 0.3357623766686237\n",
      "Clean validation loss : 0.33477092497039646\n"
     ]
    }
   ],
   "source": [
    "print(\"Training loss : {}\".format(compute_loss(y_train, tX_train, weights)))\n",
    "print(\"Validation loss : {}\".format(compute_loss(y_validation, tX_validation, weights)))\n",
    "print(\"Clean training loss : {}\".format(compute_loss(y_train_clean, tX_train_clean, weights_clean)))\n",
    "print(\"Clean validation loss : {}\".format(compute_loss(y_validation_clean, tX_validation_clean, weights_clean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:12:05.512771Z",
     "start_time": "2020-10-19T17:12:05.459039Z"
    }
   },
   "outputs": [],
   "source": [
    "from ridge_regression import *\n",
    "lambda_ = 1\n",
    "\n",
    "weights, loss = ridge_regression(y_train, tX_train, lambda_)\n",
    "weights_clean, loss_clean = ridge_regression(y_train_clean, tX_train_clean, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T17:12:07.777333Z",
     "start_time": "2020-10-19T17:12:07.737132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss : 0.3394307923481362\n",
      "Validation loss : 0.3407529146909622\n",
      "Clean training loss : 0.34000161105674515\n",
      "Clean validation loss : 0.33880983629085915\n"
     ]
    }
   ],
   "source": [
    "print(\"Training loss : {}\".format(compute_loss(y_train, tX_train, weights)))\n",
    "print(\"Validation loss : {}\".format(compute_loss(y_validation, tX_validation, weights)))\n",
    "print(\"Clean training loss : {}\".format(compute_loss(y_train_clean, tX_train_clean, weights_clean)))\n",
    "print(\"Clean validation loss : {}\".format(compute_loss(y_validation_clean, tX_validation_clean, weights_clean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T13:28:06.603353Z",
     "start_time": "2020-10-19T13:28:04.023769Z"
    }
   },
   "outputs": [],
   "source": [
    "from logistic_regression import *\n",
    "\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "initial_w_train = np.ones(tX_train.shape[1])\n",
    "initial_w_clean = np.ones(tX_clean.shape[1])\n",
    "\n",
    "weights, loss = logistic_regression(y_train, tX_train, initial_w_train, max_iters, gamma) \n",
    "weights_clean, loss_clean = logistic_regression(y_clean, tX_clean, initial_w_clean, max_iters, gamma) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T13:28:10.584800Z",
     "start_time": "2020-10-19T13:28:10.513353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss : inf\n",
      "Validation loss : inf\n",
      "Clean training loss : -8742931.734954698\n",
      "Clean validation loss : -2141131.4911447666\n"
     ]
    }
   ],
   "source": [
    "print(\"Training loss : {}\".format(calculate_loss(y_train, tX_train, weights)))\n",
    "print(\"Validation loss : {}\".format(calculate_loss(y_validation, tX_validation, weights)))\n",
    "print(\"Clean training loss : {}\".format(calculate_loss(y_train_clean, tX_train_clean, weights_clean)))\n",
    "print(\"Clean validation loss : {}\".format(calculate_loss(y_validation_clean, tX_validation_clean, weights_clean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:11:47.260205Z",
     "start_time": "2020-10-19T18:11:29.525153Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-a67f2ff1d4e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#This enable to run all cells without running this one when unnecessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_TEST_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0my_test_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_test_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids_test_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mOUTPUT_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDATA_FOLDER\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'submission.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Cours/ML/CS-433-machine-learning/project1/scripts/proj1_helpers.py\u001b[0m in \u001b[0;36mload_csv_data\u001b[0;34m(data_path, sub_sample)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding)\u001b[0m\n\u001b[1;32m   2045\u001b[0m         rows = list(\n\u001b[1;32m   2046\u001b[0m             zip(*[[conv._loose_call(_r) for _r in map(itemgetter(i), rows)]\n\u001b[0;32m-> 2047\u001b[0;31m                   for (i, conv) in enumerate(converters)]))\n\u001b[0m\u001b[1;32m   2048\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m         rows = list(\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2045\u001b[0m         rows = list(\n\u001b[1;32m   2046\u001b[0m             zip(*[[conv._loose_call(_r) for _r in map(itemgetter(i), rows)]\n\u001b[0;32m-> 2047\u001b[0;31m                   for (i, conv) in enumerate(converters)]))\n\u001b[0m\u001b[1;32m   2048\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m         rows = list(\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2044\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m         rows = list(\n\u001b[0;32m-> 2046\u001b[0;31m             zip(*[[conv._loose_call(_r) for _r in map(itemgetter(i), rows)]\n\u001b[0m\u001b[1;32m   2047\u001b[0m                   for (i, conv) in enumerate(converters)]))\n\u001b[1;32m   2048\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#This enable to run all cells without running this one when unnecessary\n",
    "if True :\n",
    "    y_test, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "    y_test_clean, tX_test_clean, ids_test_clean = clean_test(y_test, tX_test, ids_test)\n",
    "    OUTPUT_PATH = DATA_FOLDER + 'submission.csv' \n",
    "    y_pred = predict_labels(weights_clean, tX_test_clean)\n",
    "    create_csv_submission(ids_test_clean, y_pred, OUTPUT_PATH)\n",
    "else :\n",
    "    print(\"Change False to True to generate prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T18:11:47.284020Z",
     "start_time": "2020-10-19T18:11:31.948Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(y_test), len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
